{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661f2c8d-9b02-4701-894a-c9d5dfc94c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed:\n",
    "# !pip install torch torchvision transformers scikit-learn pandas pillow seaborn matplotlib\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Keep CPU responsive\n",
    "torch.set_num_threads(max(1, os.cpu_count() // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a3673c-9def-4f03-9130-b9b6f4feb4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All required CSVs found.\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Paths (aligned with your preprocessing outputs)\n",
    "    base_dir: str = r\"C:\\Users\\G ABHINAV REDDY\\Downloads\\processed_data\"\n",
    "    processed_dir_name: str = \"processed_data\"\n",
    "\n",
    "    # Files\n",
    "    memotion_csv: str = \"memotion_7k_multimodal.csv\"\n",
    "    text_train_csv: str = os.path.join(\"splits\", \"train\", \"text_train.csv\")\n",
    "    text_val_csv: str = os.path.join(\"splits\", \"val\", \"text_val.csv\")\n",
    "    text_test_csv: str = os.path.join(\"splits\", \"test\", \"text_test.csv\")\n",
    "\n",
    "    # Encoders (lightweight for CPU)\n",
    "    text_model_name: str = \"distilbert-base-uncased\"\n",
    "    max_len: int = 128\n",
    "\n",
    "    image_size: int = 224\n",
    "    batch_extract: int = 32  # CPU-friendly\n",
    "\n",
    "    # Embedding dims\n",
    "    text_dim: int = 768\n",
    "    image_dim: int = 512\n",
    "    meta_dim: int = 2  # sarcasm + humour\n",
    "\n",
    "    # Training on embeddings\n",
    "    epochs: int = 8\n",
    "    batch_train: int = 128\n",
    "    lr: float = 3e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    early_stop_patience: int = 3\n",
    "    dropout: float = 0.2\n",
    "    hidden_dim: int = 512  # small MLP\n",
    "\n",
    "    # Toggle meta flags from Memotion\n",
    "    use_meta_flags: bool = True\n",
    "\n",
    "    # Reports/checkpoints\n",
    "    reports_dirname: str = \"reports_lightweight\"\n",
    "    embeddings_dirname: str = \"embeddings_lightweight\"\n",
    "    head_ckpt: str = \"fusion_heads.pt\"\n",
    "\n",
    "    # Repro\n",
    "    seed: int = 42\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "def set_seed(seed:int=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "processed_dir = os.path.join(cfg.base_dir, cfg.processed_dir_name)\n",
    "paths = {\n",
    "    \"memotion\": os.path.join(processed_dir, cfg.memotion_csv),\n",
    "    \"text_train\": os.path.join(processed_dir, cfg.text_train_csv),\n",
    "    \"text_val\": os.path.join(processed_dir, cfg.text_val_csv),\n",
    "    \"text_test\": os.path.join(processed_dir, cfg.text_test_csv),\n",
    "}\n",
    "for k, p in paths.items():\n",
    "    assert os.path.exists(p), f\"Missing: {k} -> {p}\"\n",
    "print(\"‚úÖ All required CSVs found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff3c68b-a700-4e5a-973a-10bc939719f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_memotion_labels(off_cat: str):\n",
    "    s = str(off_cat).strip().lower()\n",
    "    # HS3\n",
    "    if s == \"hateful_offensive\":\n",
    "        hs3 = 0  # Hate\n",
    "    elif s in (\"offensive\", \"very_offensive\"):\n",
    "        hs3 = 1  # Offensive\n",
    "    elif s in (\"slight\", \"not_offensive\"):\n",
    "        hs3 = 2  # Neither\n",
    "    else:\n",
    "        hs3 = -100\n",
    "    # AB2: abusive only if offensive/very_offensive/hateful_offensive\n",
    "    ab2 = 1 if s in (\"offensive\", \"very_offensive\", \"hateful_offensive\") else 0\n",
    "    return hs3, ab2\n",
    "\n",
    "def map_text_labels(row: pd.Series):\n",
    "    # HS3 from original_class if present (e.g., Davidson dataset)\n",
    "    if \"original_class\" in row and str(row[\"original_class\"]).strip() != \"\":\n",
    "        try:\n",
    "            oc = int(float(row[\"original_class\"]))\n",
    "            hs3 = oc if oc in (0,1,2) else -100\n",
    "        except:\n",
    "            hs3 = -100\n",
    "    else:\n",
    "        hs3 = -100\n",
    "    # AB2 from binary label\n",
    "    ab2 = -100\n",
    "    if \"label\" in row and str(row[\"label\"]).strip() != \"\":\n",
    "        try:\n",
    "            v = int(float(row[\"label\"]))\n",
    "            ab2 = 1 if v == 1 else 0\n",
    "        except:\n",
    "            ab2 = -100\n",
    "    return hs3, ab2\n",
    "\n",
    "def parse_sarcasm(val) -> int:\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"sarcasm\",\"sarcastic\",\"yes\",\"true\",\"1\"} or \"sarcas\" in s:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def parse_humour(val) -> int:\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"\", \"none\", \"not_funny\", \"not funny\", \"no_humour\", \"no_humor\"}:\n",
    "        return 0\n",
    "    if any(k in s for k in [\"funny\",\"hilar\",\"humor\",\"humour\",\"very_funny\"]):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1e87bd-ea7c-46d1-9966-ac6651fd7116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9690dfaa9fef40728be669bf9715fe6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ana3\\envs\\drowsiness-detection\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\G ABHINAV REDDY\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958192920aa449efa12122b41b172453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab03a75748204d4a98c7ee4b37467ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcc841aa653479fa0b5451b7465c2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022f22a523f04846b821dcc8b7aa7fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\G ABHINAV REDDY/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:03<00:00, 13.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer + DistilBERT (frozen)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.text_model_name, use_fast=True)\n",
    "text_encoder = AutoModel.from_pretrained(cfg.text_model_name)\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "text_encoder.eval()\n",
    "\n",
    "# ResNet18 (frozen), output 512-dim\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad = False\n",
    "image_backbone = nn.Sequential(*list(resnet.children())[:-1])  # remove FC\n",
    "image_backbone.eval()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "text_encoder.to(device)\n",
    "image_backbone.to(device)\n",
    "\n",
    "img_tfm = transforms.Compose([\n",
    "    transforms.Resize((cfg.image_size, cfg.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "def mean_pool(last_hidden_state, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).float()\n",
    "    masked = last_hidden_state * mask\n",
    "    return masked.sum(1) / (mask.sum(1) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be0bbfbe-720d-4e19-89f3-fe5d3b73ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDatasetForEmbeddings(Dataset):\n",
    "    def __init__(self, csv_path, is_memotion=False, split=None):\n",
    "        self.is_memotion = is_memotion\n",
    "        df = pd.read_csv(csv_path).fillna(\"\")\n",
    "        if is_memotion and split in {\"train\",\"val\",\"test\"}:\n",
    "            rng = np.random.RandomState(123)\n",
    "            idx = np.arange(len(df))\n",
    "            rng.shuffle(idx)\n",
    "            n = len(idx); n_train = int(0.8*n); n_val = int(0.1*n)\n",
    "            splits = {\n",
    "                \"train\": idx[:n_train],\n",
    "                \"val\": idx[n_train:n_train+n_val],\n",
    "                \"test\": idx[n_train+n_val:]\n",
    "            }\n",
    "            df = df.iloc[splits[split]].reset_index(drop=True)\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        text = str(row.get(\"text\",\"\"))\n",
    "        enc = tokenizer(text, max_length=cfg.max_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # Image (Memotion)\n",
    "        img_tensor = None\n",
    "        if self.is_memotion:\n",
    "            p = str(row.get(\"image_path\",\"\")).strip()\n",
    "            if os.path.exists(p) and p != \"\":\n",
    "                try:\n",
    "                    img = Image.open(p).convert(\"RGB\")\n",
    "                    img_tensor = img_tfm(img)\n",
    "                except:\n",
    "                    img_tensor = None\n",
    "\n",
    "        if self.is_memotion:\n",
    "            hs3, ab2 = map_memotion_labels(row.get(\"offensive_category\",\"\"))\n",
    "            sarcasm = parse_sarcasm(row.get(\"sarcasm\",\"\")) if \"sarcasm\" in row else 0\n",
    "            humour = parse_humour(row.get(\"humour\",\"\")) if \"humour\" in row else 0\n",
    "        else:\n",
    "            hs3, ab2 = map_text_labels(row)\n",
    "            sarcasm, humour = 0, 0\n",
    "\n",
    "        meta = torch.tensor([sarcasm, humour], dtype=torch.float32)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"image\": img_tensor,\n",
    "            \"meta\": meta,\n",
    "            \"hs3\": hs3,\n",
    "            \"ab2\": ab2\n",
    "        }\n",
    "\n",
    "def collate_extract(batch):\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n",
    "    attention_mask = torch.stack([b[\"attention_mask\"] for b in batch])\n",
    "\n",
    "    img_idx = [i for i,b in enumerate(batch) if isinstance(b[\"image\"], torch.Tensor)]\n",
    "    images = torch.stack([batch[i][\"image\"] for i in img_idx]) if len(img_idx)>0 else None\n",
    "\n",
    "    meta = torch.stack([b[\"meta\"] for b in batch])\n",
    "    hs3 = torch.tensor([b[\"hs3\"] for b in batch], dtype=torch.long)\n",
    "    ab2 = torch.tensor([b[\"ab2\"] for b in batch], dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"images\": images,\n",
    "        \"image_indices\": torch.tensor(img_idx, dtype=torch.long) if img_idx else None,\n",
    "        \"meta\": meta,\n",
    "        \"hs3\": hs3,\n",
    "        \"ab2\": ab2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e55c3cc7-3b78-44fd-b151-e82e5e498d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dir = os.path.join(processed_dir, cfg.embeddings_dirname)\n",
    "os.makedirs(emb_dir, exist_ok=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(csv_path, is_memotion=False, split=None, tag=\"\"):\n",
    "    ds = CSVDatasetForEmbeddings(csv_path, is_memotion=is_memotion, split=split)\n",
    "    dl = DataLoader(ds, batch_size=cfg.batch_extract, shuffle=False, collate_fn=collate_extract)\n",
    "\n",
    "    all_text, all_img, all_meta, all_hs3, all_ab2 = [], [], [], [], []\n",
    "\n",
    "    for batch in dl:\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        out = text_encoder(input_ids=ids, attention_mask=mask)\n",
    "        t = mean_pool(out.last_hidden_state, mask)  # [B, 768]\n",
    "        all_text.append(t.cpu().numpy())\n",
    "\n",
    "        B = ids.size(0)\n",
    "        img_full = np.zeros((B, cfg.image_dim), dtype=np.float32)\n",
    "        if batch[\"images\"] is not None and batch[\"image_indices\"] is not None and batch[\"image_indices\"].numel()>0:\n",
    "            imgs = batch[\"images\"].to(device)\n",
    "            feats = image_backbone(imgs).flatten(1)  # [N_img, 512]\n",
    "            img_full[batch[\"image_indices\"].numpy()] = feats.cpu().numpy()\n",
    "        all_img.append(img_full)\n",
    "\n",
    "        all_meta.append(batch[\"meta\"].numpy().astype(np.float32))\n",
    "        all_hs3.append(batch[\"hs3\"].numpy())\n",
    "        all_ab2.append(batch[\"ab2\"].numpy())\n",
    "\n",
    "    X_text = np.vstack(all_text)\n",
    "    X_img  = np.vstack(all_img)\n",
    "    X_meta = np.vstack(all_meta)\n",
    "    y_hs3  = np.concatenate(all_hs3)\n",
    "    y_ab2  = np.concatenate(all_ab2)\n",
    "\n",
    "    out_path = os.path.join(emb_dir, f\"features_{tag}.npz\")\n",
    "    np.savez_compressed(out_path, X_text=X_text, X_img=X_img, X_meta=X_meta, y_hs3=y_hs3, y_ab2=y_ab2)\n",
    "    print(f\"üíæ Saved embeddings: {out_path} | shapes: text {X_text.shape}, img {X_img.shape}, meta {X_meta.shape}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db96dfdb-fa1d-4eaf-ad9f-410ea4e4cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splits\n",
    "p_train = extract_embeddings(paths[\"text_train\"], is_memotion=False, split=None, tag=\"text_train\")\n",
    "p_val   = extract_embeddings(paths[\"text_val\"],   is_memotion=False, split=None, tag=\"text_val\")\n",
    "p_test  = extract_embeddings(paths[\"text_test\"],  is_memotion=False, split=None, tag=\"text_test\")\n",
    "\n",
    "# Memotion splits (80/10/10 deterministic)\n",
    "pm_train = extract_embeddings(paths[\"memotion\"], is_memotion=True, split=\"train\", tag=\"memotion_train\")\n",
    "pm_val   = extract_embeddings(paths[\"memotion\"], is_memotion=True, split=\"val\",   tag=\"memotion_val\")\n",
    "pm_test  = extract_embeddings(paths[\"memotion\"], is_memotion=True, split=\"test\",  tag=\"memotion_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a9e7d-7c6b-406a-8737-277d879eae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz(path):\n",
    "    d = np.load(path)\n",
    "    return {k: d[k] for k in d.files}\n",
    "\n",
    "train_text = load_npz(os.path.join(emb_dir, \"features_text_train.npz\"))\n",
    "val_text   = load_npz(os.path.join(emb_dir, \"features_text_val.npz\"))\n",
    "test_text  = load_npz(os.path.join(emb_dir, \"features_text_test.npz\"))\n",
    "\n",
    "train_memo = load_npz(os.path.join(emb_dir, \"features_memotion_train.npz\"))\n",
    "val_memo   = load_npz(os.path.join(emb_dir, \"features_memotion_val.npz\"))\n",
    "test_memo  = load_npz(os.path.join(emb_dir, \"features_memotion_test.npz\"))\n",
    "\n",
    "def concat(a, b):\n",
    "    return np.vstack([a, b])\n",
    "\n",
    "def merge_sets(A, B):\n",
    "    X_text = concat(A[\"X_text\"], B[\"X_text\"])\n",
    "    X_img  = concat(A[\"X_img\"],  B[\"X_img\"])\n",
    "    X_meta = concat(A[\"X_meta\"], B[\"X_meta\"])\n",
    "    y_hs3  = np.concatenate([A[\"y_hs3\"], B[\"y_hs3\"]])\n",
    "    y_ab2  = np.concatenate([A[\"y_ab2\"], B[\"y_ab2\"]])\n",
    "    return X_text, X_img, X_meta, y_hs3, y_ab2\n",
    "\n",
    "Xtr_t, Xtr_i, Xtr_m, ytr_hs3, ytr_ab2 = merge_sets(train_text, train_memo)\n",
    "Xva_t, Xva_i, Xva_m, yva_hs3, yva_ab2 = merge_sets(val_text,   val_memo)\n",
    "Xte_t, Xte_i, Xte_m, yte_hs3, yte_ab2 = merge_sets(test_text,  test_memo)\n",
    "\n",
    "print(\"Train shapes:\", Xtr_t.shape, Xtr_i.shape, Xtr_m.shape, ytr_hs3.shape, ytr_ab2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903dd009-8e0b-4f09-8977-2c2a605d9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, X_t, X_i, X_m, y_hs3, y_ab2):\n",
    "        self.X_t = X_t.astype(np.float32)\n",
    "        self.X_i = X_i.astype(np.float32)\n",
    "        self.X_m = X_m.astype(np.float32)\n",
    "        self.y_hs3 = y_hs3.astype(np.int64)\n",
    "        self.y_ab2 = y_ab2.astype(np.int64)\n",
    "    def __len__(self): return len(self.X_t)\n",
    "    def __getitem__(self, i):\n",
    "        x = np.concatenate([\n",
    "            self.X_t[i], \n",
    "            self.X_i[i], \n",
    "            self.X_m[i] if cfg.use_meta_flags else np.zeros(cfg.meta_dim, np.float32)\n",
    "        ]).astype(np.float32)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y_hs3[i]), torch.tensor(self.y_ab2[i])\n",
    "\n",
    "train_ds = EmbeddingsDataset(Xtr_t, Xtr_i, Xtr_m, ytr_hs3, ytr_ab2)\n",
    "val_ds   = EmbeddingsDataset(Xva_t, Xva_i, Xva_m, yva_hs3, yva_ab2)\n",
    "test_ds  = EmbeddingsDataset(Xte_t, Xte_i, Xte_m, yte_hs3, yte_ab2)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=cfg.batch_train, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=cfg.batch_train, shuffle=False)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=cfg.batch_train, shuffle=False)\n",
    "\n",
    "# Compute class weights (ignore -100)\n",
    "def compute_weights(y, num_classes):\n",
    "    y = [int(v) for v in y if int(v) != -100]\n",
    "    if len(y)==0: return None\n",
    "    counts = np.bincount(y, minlength=num_classes).astype(np.float32)\n",
    "    freqs = counts / counts.sum()\n",
    "    w = 1.0 / np.clip(freqs, 1e-8, None)\n",
    "    w = w / (w.mean() + 1e-8)\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "w_hs3 = compute_weights(ytr_hs3, 3)\n",
    "w_ab2 = compute_weights(ytr_ab2, 2)\n",
    "print(\"Class weights HS3:\", None if w_hs3 is None else w_hs3.tolist())\n",
    "print(\"Class weights AB2:\", None if w_ab2 is None else w_ab2.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb4b1e4-3257-4f14-a20a-6b9872a6fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = cfg.text_dim + cfg.image_dim + (cfg.meta_dim if cfg.use_meta_flags else 0)\n",
    "\n",
    "class FusionHeads(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.hate3_head = nn.Linear(hidden//2, 3)\n",
    "        self.abuse2_head = nn.Linear(hidden//2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        return self.hate3_head(h), self.abuse2_head(h)\n",
    "\n",
    "model = FusionHeads(input_dim, hidden=cfg.hidden_dim, dropout=cfg.dropout)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "ce_hs3 = nn.CrossEntropyLoss(ignore_index=-100, weight=w_hs3)\n",
    "ce_ab2 = nn.CrossEntropyLoss(ignore_index=-100, weight=w_ab2)\n",
    "\n",
    "best_val = -1.0\n",
    "patience = 0\n",
    "ckpt_path = os.path.join(processed_dir, cfg.head_ckpt)\n",
    "\n",
    "def evaluate(loader, split=\"val\"):\n",
    "    model.eval()\n",
    "    all_hs3_y, all_hs3_p = [], []\n",
    "    all_ab2_y, all_ab2_p = [], []\n",
    "    loss_sum, n_batches = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y_hs3, y_ab2 in loader:\n",
    "            hs3_logits, ab2_logits = model(X)\n",
    "            loss = ce_hs3(hs3_logits, y_hs3) + ce_ab2(ab2_logits, y_ab2)\n",
    "            loss_sum += float(loss.item()); n_batches += 1\n",
    "\n",
    "            m_hs3 = y_hs3 != -100\n",
    "            m_ab2 = y_ab2 != -100\n",
    "            if m_hs3.any():\n",
    "                all_hs3_y.append(y_hs3[m_hs3].numpy())\n",
    "                all_hs3_p.append(hs3_logits[m_hs3].argmax(-1).numpy())\n",
    "            if m_ab2.any():\n",
    "                all_ab2_y.append(y_ab2[m_ab2].numpy())\n",
    "                all_ab2_p.append(ab2_logits[m_ab2].argmax(-1).numpy())\n",
    "\n",
    "    def agg(Y, P):\n",
    "        if len(Y)==0: return None\n",
    "        y = np.concatenate(Y); p = np.concatenate(P)\n",
    "        return dict(\n",
    "            acc=accuracy_score(y,p),\n",
    "            f1_macro=f1_score(y,p,average=\"macro\"),\n",
    "            precision_macro=precision_score(y,p,average=\"macro\",zero_division=0),\n",
    "            recall_macro=recall_score(y,p,average=\"macro\",zero_division=0),\n",
    "            y=y, p=p\n",
    "        )\n",
    "    metrics = {\n",
    "        \"loss\": loss_sum/max(n_batches,1),\n",
    "        \"hs3\": agg(all_hs3_y, all_hs3_p),\n",
    "        \"ab2\": agg(all_ab2_y, all_ab2_p)\n",
    "    }\n",
    "    def p(name, m):\n",
    "        if m is None:\n",
    "            print(f\"[{split}] {name}: no labels\")\n",
    "        else:\n",
    "            print(f\"[{split}] {name}: acc={m['acc']:.4f} f1={m['f1_macro']:.4f} prec={m['precision_macro']:.4f} rec={m['recall_macro']:.4f}\")\n",
    "    p(\"HS3\", metrics[\"hs3\"]); p(\"AB2\", metrics[\"ab2\"])\n",
    "    return metrics\n",
    "\n",
    "print(\"üèãÔ∏è Training (CPU-friendly heads)...\")\n",
    "for epoch in range(1, cfg.epochs+1):\n",
    "    model.train()\n",
    "    running = 0.0; n=0\n",
    "    for X, y_hs3, y_ab2 in train_dl:\n",
    "        hs3_logits, ab2_logits = model(X)\n",
    "        loss = ce_hs3(hs3_logits, y_hs3) + ce_ab2(ab2_logits, y_ab2)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        running += float(loss.item()); n+=1\n",
    "    print(f\"Epoch {epoch}: Train loss={running/max(n,1):.4f}\")\n",
    "    val_m = evaluate(val_dl, split=\"val\")\n",
    "\n",
    "    # early stopping on mean of available F1s (or negative loss)\n",
    "    f1s = []\n",
    "    if val_m[\"hs3\"] is not None: f1s.append(val_m[\"hs3\"][\"f1_macro\"])\n",
    "    if val_m[\"ab2\"] is not None: f1s.append(val_m[\"ab2\"][\"f1_macro\"])\n",
    "    score = float(np.mean(f1s)) if f1s else -val_m[\"loss\"]\n",
    "\n",
    "    if score > best_val:\n",
    "        best_val = score\n",
    "        patience = 0\n",
    "        torch.save({\"model\": model.state_dict(), \"cfg\": cfg.__dict__}, ckpt_path)\n",
    "        print(f\"‚úì Saved best heads to: {ckpt_path}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        print(f\"No improvement. Patience {patience}/{cfg.early_stop_patience}\")\n",
    "        if patience >= cfg.early_stop_patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "print(\"\\nüß™ Final Test Evaluation (best checkpoint)\")\n",
    "state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state[\"model\"])\n",
    "test_metrics = evaluate(test_dl, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541a559-d65e-42a7-9a58-5656f871aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_dir = os.path.join(processed_dir, cfg.reports_dirname)\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "def save_confusion(y, p, labels, title, out_png, out_csv):\n",
    "    cm = confusion_matrix(y, p, labels=list(range(len(labels))))\n",
    "    df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(title); plt.ylabel(\"True\"); plt.xlabel(\"Pred\")\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=150); plt.close()\n",
    "    df_cm.to_csv(out_csv, index=True)\n",
    "\n",
    "def dump_reports(split_name, metrics, prefix):\n",
    "    summary = {\"loss\": metrics[\"loss\"]}\n",
    "    if metrics[\"hs3\"] is not None:\n",
    "        y = metrics[\"hs3\"][\"y\"]; p = metrics[\"hs3\"][\"p\"]\n",
    "        labels = [\"Hate\",\"Offensive\",\"Neither\"]\n",
    "        rep = classification_report(y, p, target_names=labels, output_dict=True, zero_division=0)\n",
    "        pd.DataFrame(rep).transpose().to_csv(os.path.join(reports_dir, f\"{prefix}_hs3_report.csv\"))\n",
    "        save_confusion(y, p, labels, f\"{split_name.upper()} HS3\", \n",
    "                       os.path.join(reports_dir, f\"{prefix}_hs3_cm.png\"),\n",
    "                       os.path.join(reports_dir, f\"{prefix}_hs3_cm.csv\"))\n",
    "        summary[\"hs3\"] = {k: float(metrics[\"hs3\"][k]) for k in [\"acc\",\"f1_macro\",\"precision_macro\",\"recall_macro\"]}\n",
    "    else:\n",
    "        summary[\"hs3\"] = None\n",
    "\n",
    "    if metrics[\"ab2\"] is not None:\n",
    "        y = metrics[\"ab2\"][\"y\"]; p = metrics[\"ab2\"][\"p\"]\n",
    "        labels = [\"Non-abusive\",\"Abusive\"]\n",
    "        rep = classification_report(y, p, target_names=labels, output_dict=True, zero_division=0)\n",
    "        pd.DataFrame(rep).transpose().to_csv(os.path.join(reports_dir, f\"{prefix}_ab2_report.csv\"))\n",
    "        save_confusion(y, p, labels, f\"{split_name.upper()} AB2\",\n",
    "                       os.path.join(reports_dir, f\"{prefix}_ab2_cm.png\"),\n",
    "                       os.path.join(reports_dir, f\"{prefix}_ab2_cm.csv\"))\n",
    "        summary[\"ab2\"] = {k: float(metrics[\"ab2\"][k]) for k in [\"acc\",\"f1_macro\",\"precision_macro\",\"recall_macro\"]}\n",
    "    else:\n",
    "        summary[\"ab2\"] = None\n",
    "\n",
    "    with open(os.path.join(reports_dir, f\"{prefix}_summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "dump_reports(\"test\", test_metrics, \"test\")\n",
    "print(f\"üìÅ Reports saved to: {reports_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236fe60-d47c-4726-b3fb-f79347b85217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (drowsiness-detection)",
   "language": "python",
   "name": "drowsiness-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
